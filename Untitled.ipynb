{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb9ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "import matplotlib.pyplot\n",
    "import numpy\n",
    "import cv2\n",
    "import argparse\n",
    "from scipy.spatial import distance\n",
    "from collections import OrderedDict\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7f1382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.validation import check_is_fitted, column_or_1d\n",
    "import seaborn as sns\n",
    "from IPython.display import display  # Allows the use of display() for DataFrames\n",
    "from pylab import imread\n",
    "from IPython.display import Image\n",
    "from matplotlib import animation as ani\n",
    "from matplotlib_venn import venn2, venn2_circles\n",
    "from matplotlib import rc\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "import matplotlib\n",
    "import json\n",
    "import re\n",
    "import gc\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from datetime import datetime as dt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import dill\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from typing import Dict, List, Tuple, NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ea8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import maximum_filter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.ops import nearest_points\n",
    "import shapely.ops as so\n",
    "from descartes import PolygonPatch\n",
    "import scipy\n",
    "ts_conv = np.vectorize(datetime.datetime.fromtimestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "969299b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 18:18:32.785624: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 18:18:34.024697: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ines/anaconda3/envs/tf/lib/python3.9/site-packages/cv2/../../../../lib::/home/ines/anaconda3/envs/tf/lib/\n",
      "2023-03-04 18:18:34.024811: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ines/anaconda3/envs/tf/lib/python3.9/site-packages/cv2/../../../../lib::/home/ines/anaconda3/envs/tf/lib/\n",
      "2023-03-04 18:18:34.024821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa5e5b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas settings -----------------------------------------\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "\n",
    "# Graph drawing -------------------------------------------\n",
    "\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\", color_codes=True)\n",
    "sns.set_style(\"whitegrid\", {'grid.linestyle': '--'})\n",
    "red = sns.xkcd_rgb[\"light red\"]\n",
    "green = sns.xkcd_rgb[\"medium green\"]\n",
    "blue = sns.xkcd_rgb[\"denim blue\"]\n",
    "\n",
    "\n",
    "# ML -------------------------------------------\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a902d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_dump(obj, path):\n",
    "    with open(path, mode='wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def pickle_load(path):\n",
    "    with open(path, mode='rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "\n",
    "def pickle_dump_dill(obj, path):\n",
    "    with open(path, mode='wb') as f:\n",
    "        dill.dump(obj, f)\n",
    "\n",
    "\n",
    "def pickle_load_dill(path):\n",
    "    with open(path, mode='rb') as f:\n",
    "        data = dill.load(f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60df2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            # skip datetime type or categorical type\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(\n",
    "        100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd49a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c12567f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0563690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureStore():\n",
    "\n",
    "    # necessayr to re-check\n",
    "    floor_convert = {'1F':  0, '2F': 1, '3F': 2, '4F': 3, '5F': 4,\n",
    "                     '6F': 5, '7F': 6, '8F': 7, '9F': 8,\n",
    "                     'B': -1, 'B1': -1, 'B2': -2, 'B3': -3,\n",
    "                     'BF': -1, 'BM': -1,\n",
    "                     'F1': 0, 'F2': 1, 'F3': 2, 'F4': 3, 'F5': 4,\n",
    "                     'F6': 5, 'F7': 6, 'F8': 7, 'F9': 8, 'F10': 9,\n",
    "                     'L1': 0, 'L2': 1, 'L3': 2, 'L4': 3, 'L5': 4,\n",
    "                     'L6': 5, 'L7': 6, 'L8': 7, 'L9': 8, 'L10': 9,\n",
    "                     'L11': 10,\n",
    "                     'G': 0, 'LG1': 0, 'LG2': 1, 'LM': 0, 'M': 0,\n",
    "                     'P1': 0, 'P2': 1, }\n",
    "\n",
    "    df_types = ['accelerometer',\n",
    "                'accelerometer_uncalibrated',\n",
    "                'beacon',\n",
    "                'gyroscope',\n",
    "                'gyroscope_uncalibrated',\n",
    "                'magnetic_field',\n",
    "                'magnetic_field_uncalibrated',\n",
    "                'rotation_vector',\n",
    "                'waypoint',\n",
    "                'wifi']\n",
    "\n",
    "    # https://github.com/location-competition/indoor-location-competition-20\n",
    "    df_type_cols = {'accelerometer': [\"timestamp\", \"x\", \"y\", \"z\", \"accuracy\"],\n",
    "                    'accelerometer_uncalibrated': [\"timestamp\", \"x\", \"y\", \"z\",\n",
    "                                                   \"x2\", \"y2\", \"z2\", \"accuracy\"],\n",
    "                    'beacon': [\"timestamp\", \"uuid\", \"major_id\", \"minor_id\", \"tx_power\",\n",
    "                               \"rssi\", \"distance\", \"mac_addr\", \"timestamp2\"],\n",
    "                    'gyroscope': [\"timestamp\", \"x\", \"y\", \"z\", \"accuracy\"],\n",
    "                    'gyroscope_uncalibrated': [\"timestamp\", \"x\", \"y\", \"z\",\n",
    "                                               \"x2\", \"y2\", \"z2\", \"accuracy\"],\n",
    "                    'magnetic_field': [\"timestamp\", \"x\", \"y\", \"z\", \"accuracy\"],\n",
    "                    'magnetic_field_uncalibrated': [\"timestamp\", \"x\", \"y\", \"z\",\n",
    "                                                    \"x2\", \"y2\", \"z2\", \"accuracy\"],\n",
    "                    'rotation_vector': [\"timestamp\", \"x\", \"y\", \"z\", \"accuracy\"],\n",
    "                    'waypoint': [\"timestamp\", \"x\", \"y\"],\n",
    "                    'wifi': [\"timestamp\", \"ssid\", \"bssid\", \"rssi\", \"frequency\",\n",
    "                             \"last_seen_timestamp\", ]}\n",
    "\n",
    "    dtype_dict = {}\n",
    "    dtype_dict[\"accelerometer\"] = {\"timestamp\": int, \"x\": float, \"y\": float, \"z\": float,\n",
    "                                   \"accuracy\": int}\n",
    "    dtype_dict[\"accelerometer_uncalibrated\"] = {\"timestamp\": int, \"x\": float, \"y\": float,\n",
    "                                                \"z\": float, \"x2\": float, \"y2\": float,\n",
    "                                                \"z2\": float, \"accuracy\": int}\n",
    "    dtype_dict[\"beacon\"] = {\"timestamp\": int, \"uuid\": str, \"major_id\": str,\n",
    "                            \"minor_id\": str, \"tx_power\": int,  \"rssi\": int,\n",
    "                            \"distance\": float, \"mac_addr\": str, \"timestamp2\": int}\n",
    "    dtype_dict[\"gyroscope\"] = {\"timestamp\": int, \"x\": float, \"y\": float, \"z\": float,\n",
    "                               \"accuracy\": int}\n",
    "    dtype_dict[\"gyroscope_uncalibrated\"] = {\"timestamp\": int, \"x\": float, \"y\": float,\n",
    "                                            \"z\": float, \"x2\": float, \"y2\": float,\n",
    "                                            \"z2\": float, \"accuracy\": int}\n",
    "    dtype_dict[\"magnetic_field\"] = {\"timestamp\": int, \"x\": float, \"y\": float,\n",
    "                                    \"z\": float, \"accuracy\": int}\n",
    "    dtype_dict[\"magnetic_field_uncalibrated\"] = {\"timestamp\": int, \"x\": float,\n",
    "                                                 \"y\": float, \"z\": float, \"x2\": float,\n",
    "                                                 \"y2\": float, \"z2\": float, \"accuracy\": int}\n",
    "    dtype_dict[\"rotation_vector\"] = {\"timestamp\": int, \"x\": float, \"y\": float,\n",
    "                                     \"z\": float, \"accuracy\": int}\n",
    "    dtype_dict[\"waypoint\"] = {\"timestamp\": int,\n",
    "                              \"x\": float, \"y\": float, \"z\": float}\n",
    "    dtype_dict[\"wifi\"] = {\"timestamp\": int, \"ssid\": str, \"bssid\": str,\n",
    "                          \"rssi\": int, \"frequency\": int, \"last_seen_timestamp\": int}\n",
    "\n",
    "    def __init__(self, site_id, floor, path_id,\n",
    "                 input_path=\"../input/indoor-location-navigation/\",\n",
    "                 save_path=\"../mid\"):\n",
    "        self.site_id = site_id.strip()\n",
    "        self.floor = floor.strip()\n",
    "        self.n_floor = self.floor_convert[self.floor]\n",
    "        self.path_id = path_id.strip()\n",
    "\n",
    "        self.input_path = input_path\n",
    "        assert Path(input_path).exists(\n",
    "        ), f\"input_path do not exist: {input_path}\"\n",
    "\n",
    "        self.save_path = save_path\n",
    "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.site_info = SiteInfo(\n",
    "            site_id=self.site_id, floor=self.floor, input_path=self.input_path)\n",
    "\n",
    "    def _flatten(self, l):\n",
    "        return list(itertools.chain.from_iterable(l))\n",
    "\n",
    "    def multi_line_spliter(self, s):\n",
    "        matches = re.finditer(\"TYPE_\", s)\n",
    "        matches_positions = [match.start() for match in matches]\n",
    "        split_idx = [0] + [matches_positions[i] -\n",
    "                           14 for i in range(1, len(matches_positions))] + [len(s)]\n",
    "        return [s[split_idx[i]:split_idx[i+1]] for i in range(len(split_idx)-1)]\n",
    "\n",
    "    def load_df(self, ):\n",
    "        path = str(Path(self.input_path) /\n",
    "                   f\"train/{self.site_id}/{self.floor}/{self.path_id}.txt\")\n",
    "        with open(path) as f:\n",
    "            data = f.readlines()\n",
    "\n",
    "        modified_data = []\n",
    "        for s in data:\n",
    "            if s.count(\"TYPE_\") > 1:\n",
    "                lines = self.multi_line_spliter(s)\n",
    "                modified_data.extend(lines)\n",
    "            else:\n",
    "                modified_data.append(s)\n",
    "        del data\n",
    "        self.meta_info_len = len([d for d in modified_data if d[0] == \"#\"])\n",
    "        self.meta_info_df = pd.DataFrame([m.replace(\"\\n\", \"\").split(\":\")\n",
    "                                          for m in self._flatten([d.split(\"\\t\")\n",
    "                                                                  for d in modified_data if d[0] == \"#\"]) if m != \"#\"])\n",
    "\n",
    "        data_df = pd.DataFrame([d.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "                                for d in modified_data if d[0] != \"#\"])\n",
    "        for dt in self.df_types:\n",
    "            # select data type\n",
    "            df_s = data_df[data_df[1] == f\"TYPE_{dt.upper()}\"]\n",
    "            if len(df_s) == 0:\n",
    "                setattr(self, dt, pd.DataFrame(columns=self.df_type_cols[dt]))\n",
    "            else:\n",
    "                # remove empty cols\n",
    "                na_info = df_s.isna().sum(axis=0) == len(df_s)\n",
    "                df_s = df_s[[i for i in na_info[na_info ==\n",
    "                                                False].index if i != 1]].reset_index(drop=True)\n",
    "\n",
    "                if len(df_s.columns) != len(self.df_type_cols[dt]):\n",
    "                    df_s.columns = self.df_type_cols[dt][:len(df_s.columns)]\n",
    "                else:\n",
    "                    df_s.columns = self.df_type_cols[dt]\n",
    "\n",
    "                # set dtype\n",
    "                for c in df_s.columns:\n",
    "                    df_s[c] = df_s[c].astype(self.dtype_dict[dt][c])\n",
    "\n",
    "                # set DataFrame to attr\n",
    "                setattr(self, dt, df_s)\n",
    "\n",
    "    def get_site_info(self, keep_raw=False):\n",
    "        self.site_info.get_site_info(keep_raw=keep_raw)\n",
    "\n",
    "    def load_all_data(self, keep_raw=False):\n",
    "        self.load_df()\n",
    "        self.get_site_info(keep_raw=keep_raw)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if item in self.df_types:\n",
    "            return getattr(self, item)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def save(self, ):\n",
    "        # to be implemented\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "167199ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiteInfo():\n",
    "    def __init__(self, site_id, floor, input_path=\"../input/indoor-location-navigation/\"):\n",
    "        self.site_id = site_id\n",
    "        self.floor = floor\n",
    "        self.input_path = input_path\n",
    "        assert Path(input_path).exists(\n",
    "        ), f\"input_path do not exist: {input_path}\"\n",
    "\n",
    "    def get_site_info(self, keep_raw=False):\n",
    "        floor_info_path = f\"{self.input_path}/metadata/{self.site_id}/{self.floor}/floor_info.json\"\n",
    "        with open(floor_info_path, \"r\") as f:\n",
    "            self.floor_info = json.loads(f.read())\n",
    "            self.site_height = self.floor_info[\"map_info\"][\"height\"]\n",
    "            self.site_width = self.floor_info[\"map_info\"][\"width\"]\n",
    "            if not keep_raw:\n",
    "                del self.floor_info\n",
    "\n",
    "        geojson_map_path = f\"{self.input_path}/metadata/{self.site_id}/{self.floor}/geojson_map.json\"\n",
    "        with open(geojson_map_path, \"r\") as f:\n",
    "            self.geojson_map = json.loads(f.read())\n",
    "            self.map_type = self.geojson_map[\"type\"]\n",
    "            self.features = self.geojson_map[\"features\"]\n",
    "\n",
    "            self.floor_coordinates = self.features[0][\"geometry\"][\"coordinates\"]\n",
    "            self.store_coordinates = [self.features[i][\"geometry\"][\"coordinates\"]\n",
    "                                      for i in range(1, len(self.features))]\n",
    "\n",
    "            if not keep_raw:\n",
    "                del self.geojson_map\n",
    "\n",
    "    def show_site_image(self):\n",
    "        path = f\"{self.input_path}/metadata/{self.site_id}/{self.floor}/floor_image.png\"\n",
    "        plt.imshow(imread(path), extent=[\n",
    "                   0, self.site_width, 0, self.site_height])\n",
    "\n",
    "    def draw_polygon(self, size=8, only_floor=False):\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = plt.subplot(111)\n",
    "\n",
    "        xmax, xmin, ymax, ymin = self._draw(\n",
    "            self.floor_coordinates, ax, calc_minmax=True)\n",
    "        if not only_floor:\n",
    "            self._draw(self.store_coordinates, ax, fill=True)\n",
    "        plt.legend([])\n",
    "\n",
    "        xrange = xmax - xmin\n",
    "        yrange = ymax - ymin\n",
    "        ratio = yrange / xrange\n",
    "\n",
    "        self.x_size = size\n",
    "        self.y_size = size*ratio\n",
    "\n",
    "        fig.set_figwidth(size)\n",
    "        fig.set_figheight(size*ratio)\n",
    "        # plt.show()\n",
    "        return ax\n",
    "\n",
    "    def _draw(self, coordinates, ax, fill=False, calc_minmax=False):\n",
    "        xmax, ymax = -np.inf, -np.inf\n",
    "        xmin, ymin = np.inf, np.inf\n",
    "        for i in range(len(coordinates)):\n",
    "            ndim = np.ndim(coordinates[i])\n",
    "            if ndim == 2:\n",
    "                corrd_df = pd.DataFrame(coordinates[i])\n",
    "                if fill:\n",
    "                    ax.fill(corrd_df[0], corrd_df[1], alpha=0.7)\n",
    "                else:\n",
    "                    corrd_df.plot.line(x=0, y=1, style=\"-\", ax=ax)\n",
    "\n",
    "                if calc_minmax:\n",
    "                    xmax = max(xmax, corrd_df[0].max())\n",
    "                    xmin = min(xmin, corrd_df[0].min())\n",
    "\n",
    "                    ymax = max(ymax, corrd_df[1].max())\n",
    "                    ymin = min(ymin, corrd_df[1].min())\n",
    "            elif ndim == 3:\n",
    "                for j in range(len(coordinates[i])):\n",
    "                    corrd_df = pd.DataFrame(coordinates[i][j])\n",
    "                    if fill:\n",
    "                        ax.fill(corrd_df[0], corrd_df[1], alpha=0.6)\n",
    "                    else:\n",
    "                        corrd_df.plot.line(x=0, y=1, style=\"-\", ax=ax)\n",
    "\n",
    "                    if calc_minmax:\n",
    "                        xmax = max(xmax, corrd_df[0].max())\n",
    "                        xmin = min(xmin, corrd_df[0].min())\n",
    "\n",
    "                        ymax = max(ymax, corrd_df[1].max())\n",
    "                        ymin = min(ymin, corrd_df[1].min())\n",
    "            else:\n",
    "                assert False, f\"ndim of coordinates should be 2 or 3: {ndim}\"\n",
    "        if calc_minmax:\n",
    "            return xmax, xmin, ymax, ymin\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../input/indoor-location-navigation/'\n",
    "mydata_dir = '../input/indoordataset/'\n",
    "out_dir = '../input/indoor-public/features/'\n",
    "# train_meta_data\n",
    "train_meta = glob(data_dir+\"train/*/*/*\")\n",
    "train_meta_org = pd.DataFrame(train_meta)\n",
    "train_meta = train_meta_org[0].str.split(\"/\", expand=True)[[4, 5, 6]]\n",
    "train_meta.columns = [\"site_id\", \"floor\", \"path_id\"]\n",
    "train_meta[\"path_id\"] = train_meta[\"path_id\"].str.replace(\".txt\", \"\")\n",
    "train_meta[\"path\"] = train_meta_org[0]\n",
    "train_meta.head()\n",
    "\n",
    "\n",
    "# test_meta_data\n",
    "sample_sub = pd.read_csv(data_dir+'sample_submission.csv')\n",
    "test_meta = sample_sub[\"site_path_timestamp\"].apply(\n",
    "    lambda x: pd.Series(x.split(\"_\")))\n",
    "test_meta.columns = [\"site_id\", \"path_id\", \"timestamp\"]\n",
    "test_meta = test_meta.drop('timestamp', axis=1)\n",
    "test_meta = test_meta.drop_duplicates(\n",
    "    subset=[\"site_id\", \"path_id\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f545d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file):\n",
    "    with open(file) as f:\n",
    "        txt = f.readlines()\n",
    "\n",
    "    modified_data = []\n",
    "    for s in txt:\n",
    "        if s.count(\"TYPE_\") > 1:\n",
    "            lines = multi_line_spliter(s)\n",
    "            modified_data.extend(lines)\n",
    "        else:\n",
    "            modified_data.append(s)\n",
    "    return modified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55755d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flatten(l):\n",
    "    return list(itertools.chain.from_iterable(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b9f41a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_feature_test(site_id, path_id, input_path, sample_sub):\n",
    "    file = f\"{input_path}/test/{path_id}.txt\"\n",
    "    content = read_txt(file)\n",
    "    data_df = pd.DataFrame([d.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "                            for d in content if d[0] != \"#\"])\n",
    "    data_dict = OrderedDict()\n",
    "    for dt in FeatureStore.df_types:\n",
    "        # select data type\n",
    "        df_s = data_df[data_df[1] == f\"TYPE_{dt.upper()}\"]\n",
    "        if len(df_s) == 0:\n",
    "            setattr(data_dict, dt, pd.DataFrame(\n",
    "                columns=FeatureStore.df_type_cols[dt]))\n",
    "        else:\n",
    "            # remove empty cols\n",
    "            na_info = df_s.isna().sum(axis=0) == len(df_s)\n",
    "            df_s = df_s[[i for i in na_info[na_info ==\n",
    "                                            False].index if i != 1]].reset_index(drop=True)\n",
    "\n",
    "            if len(df_s.columns) != len(FeatureStore.df_type_cols[dt]):\n",
    "                df_s.columns = FeatureStore.df_type_cols[dt][:len(\n",
    "                    df_s.columns)]\n",
    "            else:\n",
    "                df_s.columns = FeatureStore.df_type_cols[dt]\n",
    "\n",
    "            # set dtype\n",
    "            for c in df_s.columns:\n",
    "                df_s[c] = df_s[c].astype(FeatureStore.dtype_dict[dt][c])\n",
    "            setattr(data_dict, dt, df_s)\n",
    "    data_dict.meta_info_df = pd.DataFrame([m.replace(\"\\n\", \"\").split(\":\")\n",
    "                                           for m in _flatten([d.split(\"\\t\")\n",
    "                                                              for d in content if d[0] == \"#\"]) if m != \"#\"])\n",
    "    data_dict.meta_info_df[data_dict.meta_info_df[0] == 'startTime'][1] = int(\n",
    "        data_dict.meta_info_df[0][int(np.where(data_dict.meta_info_df[0] == 'startTime')[0]+1)])\n",
    "    data_dict.meta_info_df[data_dict.meta_info_df[0] == 'endTime'][1] = int(\n",
    "        data_dict.meta_info_df[0][int(np.where(data_dict.meta_info_df[0] == 'endTime')[0]+1)])\n",
    "\n",
    "    data_dict.waypoint['timestamp'] = sample_sub[sample_sub.path_id ==\n",
    "                                                 path_id].timestamp.values.astype(int)\n",
    "    data_dict.waypoint['x'] = 0\n",
    "    data_dict.waypoint['y'] = 0\n",
    "    data_dict.n_floor = 0\n",
    "    data_dict.site_id = site_id\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c634f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_feature_pickle=False\n",
    "if create_feature_pickle:\n",
    "    print('create_feature_pickle')\n",
    "    for i in tqdm(range(len(train_meta))):\n",
    "        t = train_meta.iloc[i]\n",
    "        #print(f\"site_id: {t.site_id}, floor: {t.floor}, path_id: {t.path_id}\")\n",
    "        feature = FeatureStore(site_id=t.site_id, floor=t.floor,\n",
    "                               path_id=t.path_id, input_path=data_dir)\n",
    "        feature.load_all_data()\n",
    "        pickle_dump_dill(feature, out_dir+'feature/'+t.path_id+'.pickle')\n",
    "\n",
    "    for i in tqdm(range(len(test_meta))):\n",
    "        t = test_meta.iloc[i]\n",
    "        #print(f\"site_id: {t.site_id}, floor: {t.floor}, path_id: {t.path_id}\")\n",
    "        feature = get_feature_test(\n",
    "            site_id=t.site_id, path_id=t.path_id, input_path=data_dir, sample_sub=sample_sub)\n",
    "        print(feature.meta_info_df[0][1])\n",
    "        pickle_dump_dill(feature, out_dir+'feature/'+t.path_id+'.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cd04e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_count_rank(df, column):\n",
    "    count_rank = df.groupby(column)[column].count().rank(\n",
    "        ascending=False, method='first')\n",
    "    df[column+'_count_rank'] = df[column].map(count_rank)\n",
    "    #df[column+'_count_rank'] = df[column+'_count_rank'].astype(np.int32)\n",
    "    return df, count_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "579b5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_count_rank(df, count_rank_dict, column, mode='convert'):\n",
    "    if mode == 'convert':\n",
    "        df[column] = df[column].map(count_rank_dict.get)\n",
    "    elif mode == 'add':\n",
    "        df[column+'_count_rank'] = df[column].map(count_rank_dict.get)\n",
    "    #df[column+'_count_rank'] = df[column+'_count_rank'].astype(np.int32)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c249023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_left_merge(df1, df2, key='timestamp'):\n",
    "    df2 = df2.set_index(key)\n",
    "    df = pd.concat([df1.reset_index(drop=True), df2.reindex(\n",
    "        df1[key].values).reset_index(drop=True)], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "270e0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate_wifi(wifi_df, last_seen_timestamp_margin=10):\n",
    "    old_bssid = ''\n",
    "    old_rssi = -9999\n",
    "    old_last_seen_timestamp = -9999\n",
    "    if len(wifi_df) > 0:\n",
    "        new_rows = []\n",
    "        wifi_df = wifi_df.sort_values(['bssid', 'timestamp'])\n",
    "        for cnt, row in enumerate(wifi_df[['timestamp', 'ssid', 'bssid', 'rssi', 'frequency', 'last_seen_timestamp']].values):\n",
    "            bssid = str(row[2])\n",
    "            rssi = int(row[3])\n",
    "            last_seen_timestamp = int(row[4])\n",
    "            if (bssid != old_bssid) or (rssi != old_rssi) or (abs(last_seen_timestamp-old_last_seen_timestamp) > last_seen_timestamp_margin):\n",
    "                new_rows.append(row)\n",
    "            old_bssid = bssid\n",
    "            old_rssi = rssi\n",
    "            old_last_seen_timestamp = last_seen_timestamp\n",
    "        new_rows = np.vstack(new_rows)\n",
    "        new_wifi_df = pd.DataFrame(new_rows, columns=[\n",
    "                                   'timestamp', 'ssid', 'bssid', 'rssi', 'frequency', 'last_seen_timestamp'])\n",
    "        new_wifi_df = new_wifi_df.sort_values('timestamp')\n",
    "    else:\n",
    "        new_wifi_df = wifi_df\n",
    "    return new_wifi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "636599e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ts_seq(ts_seq, sep_ts):\n",
    "    \"\"\"\n",
    "    :param ts_seq:\n",
    "    :param sep_ts:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tss = ts_seq[:, 0].astype(float)\n",
    "    unique_sep_ts = np.unique(sep_ts)\n",
    "    ts_seqs = []\n",
    "    start_index = 0\n",
    "    for i in range(0, unique_sep_ts.shape[0]):\n",
    "        end_index = np.searchsorted(tss, unique_sep_ts[i], side='right')\n",
    "        if start_index == end_index:\n",
    "            continue\n",
    "        ts_seqs.append(ts_seq[start_index:end_index, :].copy())\n",
    "        start_index = end_index\n",
    "\n",
    "    # tail data\n",
    "    if start_index < ts_seq.shape[0]:\n",
    "        ts_seqs.append(ts_seq[start_index:, :].copy())\n",
    "\n",
    "    return ts_seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59cae1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_trajectory(original_xys, end_xy):\n",
    "    \"\"\"\n",
    "    :param original_xys: numpy ndarray, shape(N, 2)\n",
    "    :param end_xy: numpy ndarray, shape(1, 2)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corrected_xys = np.zeros((0, 2))\n",
    "\n",
    "    A = original_xys[0, :]\n",
    "    B = end_xy\n",
    "    Bp = original_xys[-1, :]\n",
    "\n",
    "    angle_BAX = np.arctan2(B[1] - A[1], B[0] - A[0])\n",
    "    angle_BpAX = np.arctan2(Bp[1] - A[1], Bp[0] - A[0])\n",
    "    angle_BpAB = angle_BpAX - angle_BAX\n",
    "    AB = np.sqrt(np.sum((B - A) ** 2))\n",
    "    ABp = np.sqrt(np.sum((Bp - A) ** 2))\n",
    "\n",
    "    corrected_xys = np.append(corrected_xys, [A], 0)\n",
    "    for i in np.arange(1, np.size(original_xys, 0)):\n",
    "        angle_CpAX = np.arctan2(\n",
    "            original_xys[i, 1] - A[1], original_xys[i, 0] - A[0])\n",
    "\n",
    "        angle_CAX = angle_CpAX - angle_BpAB\n",
    "\n",
    "        ACp = np.sqrt(np.sum((original_xys[i, :] - A) ** 2))\n",
    "\n",
    "        AC = ACp * AB / ABp\n",
    "\n",
    "        delta_C = np.array([AC * np.cos(angle_CAX), AC * np.sin(angle_CAX)])\n",
    "\n",
    "        C = delta_C + A\n",
    "\n",
    "        corrected_xys = np.append(corrected_xys, [C], 0)\n",
    "\n",
    "    return corrected_xys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c0ae29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_positions(rel_positions, reference_positions):\n",
    "    \"\"\"\n",
    "    :param rel_positions:\n",
    "    :param reference_positions:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    rel_positions_list = split_ts_seq(rel_positions, reference_positions[:, 0])\n",
    "    if len(rel_positions_list) != reference_positions.shape[0] - 1:\n",
    "        # print(f'Rel positions list size: {len(rel_positions_list)}, ref positions size: {reference_positions.shape[0]}')\n",
    "        del rel_positions_list[-1]\n",
    "    assert len(rel_positions_list) == reference_positions.shape[0] - 1\n",
    "\n",
    "    corrected_positions = np.zeros((0, 3))\n",
    "    for i, rel_ps in enumerate(rel_positions_list):\n",
    "        start_position = reference_positions[i]\n",
    "        end_position = reference_positions[i + 1]\n",
    "        abs_ps = np.zeros(rel_ps.shape)\n",
    "        abs_ps[:, 0] = rel_ps[:, 0]\n",
    "        # abs_ps[:, 1:3] = rel_ps[:, 1:3] + start_position[1:3]\n",
    "        abs_ps[0, 1:3] = rel_ps[0, 1:3] + start_position[1:3]\n",
    "        for j in range(1, rel_ps.shape[0]):\n",
    "            abs_ps[j, 1:3] = abs_ps[j-1, 1:3] + rel_ps[j, 1:3]\n",
    "        abs_ps = np.insert(abs_ps, 0, start_position, axis=0)\n",
    "        corrected_xys = correct_trajectory(abs_ps[:, 1:3], end_position[1:3])\n",
    "        corrected_ps = np.column_stack((abs_ps[:, 0], corrected_xys))\n",
    "        if i == 0:\n",
    "            corrected_positions = np.append(\n",
    "                corrected_positions, corrected_ps, axis=0)\n",
    "        else:\n",
    "            corrected_positions = np.append(\n",
    "                corrected_positions, corrected_ps[1:], axis=0)\n",
    "\n",
    "    corrected_positions = np.array(corrected_positions)\n",
    "\n",
    "    return corrected_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5755c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters_filter(sample_freq, warmup_data, cut_off_freq=2):\n",
    "    order = 4\n",
    "    filter_b, filter_a = signal.butter(\n",
    "        order, cut_off_freq / (sample_freq / 2), 'low', False)\n",
    "    zf = signal.lfilter_zi(filter_b, filter_a)\n",
    "    _, zf = signal.lfilter(filter_b, filter_a, warmup_data, zi=zf)\n",
    "    _, filter_zf = signal.lfilter(filter_b, filter_a, warmup_data, zi=zf)\n",
    "\n",
    "    return filter_b, filter_a, filter_zf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af591c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotation_matrix_from_vector(rotation_vector):\n",
    "    q1 = rotation_vector[0]\n",
    "    q2 = rotation_vector[1]\n",
    "    q3 = rotation_vector[2]\n",
    "\n",
    "    if rotation_vector.size >= 4:\n",
    "        q0 = rotation_vector[3]\n",
    "    else:\n",
    "        q0 = 1 - q1*q1 - q2*q2 - q3*q3\n",
    "        if q0 > 0:\n",
    "            q0 = np.sqrt(q0)\n",
    "        else:\n",
    "            q0 = 0\n",
    "\n",
    "    sq_q1 = 2 * q1 * q1\n",
    "    sq_q2 = 2 * q2 * q2\n",
    "    sq_q3 = 2 * q3 * q3\n",
    "    q1_q2 = 2 * q1 * q2\n",
    "    q3_q0 = 2 * q3 * q0\n",
    "    q1_q3 = 2 * q1 * q3\n",
    "    q2_q0 = 2 * q2 * q0\n",
    "    q2_q3 = 2 * q2 * q3\n",
    "    q1_q0 = 2 * q1 * q0\n",
    "\n",
    "    R = np.zeros((9,))\n",
    "    if R.size == 9:\n",
    "        R[0] = 1 - sq_q2 - sq_q3\n",
    "        R[1] = q1_q2 - q3_q0\n",
    "        R[2] = q1_q3 + q2_q0\n",
    "\n",
    "        R[3] = q1_q2 + q3_q0\n",
    "        R[4] = 1 - sq_q1 - sq_q3\n",
    "        R[5] = q2_q3 - q1_q0\n",
    "\n",
    "        R[6] = q1_q3 - q2_q0\n",
    "        R[7] = q2_q3 + q1_q0\n",
    "        R[8] = 1 - sq_q1 - sq_q2\n",
    "\n",
    "        R = np.reshape(R, (3, 3))\n",
    "    elif R.size == 16:\n",
    "        R[0] = 1 - sq_q2 - sq_q3\n",
    "        R[1] = q1_q2 - q3_q0\n",
    "        R[2] = q1_q3 + q2_q0\n",
    "        R[3] = 0.0\n",
    "\n",
    "        R[4] = q1_q2 + q3_q0\n",
    "        R[5] = 1 - sq_q1 - sq_q3\n",
    "        R[6] = q2_q3 - q1_q0\n",
    "        R[7] = 0.0\n",
    "\n",
    "        R[8] = q1_q3 - q2_q0\n",
    "        R[9] = q2_q3 + q1_q0\n",
    "        R[10] = 1 - sq_q1 - sq_q2\n",
    "        R[11] = 0.0\n",
    "\n",
    "        R[12] = R[13] = R[14] = 0.0\n",
    "        R[15] = 1.0\n",
    "\n",
    "        R = np.reshape(R, (4, 4))\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee197009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orientation(R):\n",
    "    flat_R = R.flatten()\n",
    "    values = np.zeros((3,))\n",
    "    if np.size(flat_R) == 9:\n",
    "        values[0] = np.arctan2(flat_R[1], flat_R[4])\n",
    "        values[1] = np.arcsin(-flat_R[7])\n",
    "        values[2] = np.arctan2(-flat_R[6], flat_R[8])\n",
    "    else:\n",
    "        values[0] = np.arctan2(flat_R[1], flat_R[5])\n",
    "        values[1] = np.arcsin(-flat_R[9])\n",
    "        values[2] = np.arctan2(-flat_R[8], flat_R[10])\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "705e674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_steps(acce_datas):\n",
    "    step_timestamps = np.array([])\n",
    "    step_indexs = np.array([], dtype=int)\n",
    "    step_acce_max_mins = np.zeros((0, 4))\n",
    "    sample_freq = 50\n",
    "    window_size = 22\n",
    "    low_acce_mag = 0.6\n",
    "    step_criterion = 1\n",
    "    interval_threshold = 250\n",
    "\n",
    "    acce_max = np.zeros((2,))\n",
    "    acce_min = np.zeros((2,))\n",
    "    acce_binarys = np.zeros((window_size,), dtype=int)\n",
    "    acce_mag_pre = 0\n",
    "    state_flag = 0\n",
    "\n",
    "    warmup_data = np.ones((window_size,)) * 9.81\n",
    "    filter_b, filter_a, filter_zf = init_parameters_filter(\n",
    "        sample_freq, warmup_data)\n",
    "    acce_mag_window = np.zeros((window_size, 1))\n",
    "\n",
    "    # detect steps according to acceleration magnitudes\n",
    "    for i in np.arange(0, np.size(acce_datas, 0)):\n",
    "        acce_data = acce_datas[i, :]\n",
    "        acce_mag = np.sqrt(np.sum(acce_data[1:] ** 2))\n",
    "\n",
    "        acce_mag_filt, filter_zf = signal.lfilter(\n",
    "            filter_b, filter_a, [acce_mag], zi=filter_zf)\n",
    "        acce_mag_filt = acce_mag_filt[0]\n",
    "\n",
    "        acce_mag_window = np.append(acce_mag_window, [acce_mag_filt])\n",
    "        acce_mag_window = np.delete(acce_mag_window, 0)\n",
    "        mean_gravity = np.mean(acce_mag_window)\n",
    "        acce_std = np.std(acce_mag_window)\n",
    "        mag_threshold = np.max([low_acce_mag, 0.4 * acce_std])\n",
    "\n",
    "        # detect valid peak or valley of acceleration magnitudes\n",
    "        acce_mag_filt_detrend = acce_mag_filt - mean_gravity\n",
    "        if acce_mag_filt_detrend > np.max([acce_mag_pre, mag_threshold]):\n",
    "            # peak\n",
    "            acce_binarys = np.append(acce_binarys, [1])\n",
    "            acce_binarys = np.delete(acce_binarys, 0)\n",
    "        elif acce_mag_filt_detrend < np.min([acce_mag_pre, -mag_threshold]):\n",
    "            # valley\n",
    "            acce_binarys = np.append(acce_binarys, [-1])\n",
    "            acce_binarys = np.delete(acce_binarys, 0)\n",
    "        else:\n",
    "            # between peak and valley\n",
    "            acce_binarys = np.append(acce_binarys, [0])\n",
    "            acce_binarys = np.delete(acce_binarys, 0)\n",
    "\n",
    "        if (acce_binarys[-1] == 0) and (acce_binarys[-2] == 1):\n",
    "            if state_flag == 0:\n",
    "                acce_max[:] = acce_data[0], acce_mag_filt\n",
    "                state_flag = 1\n",
    "            elif (state_flag == 1) and ((acce_data[0] - acce_max[0]) <= interval_threshold) and (\n",
    "                    acce_mag_filt > acce_max[1]):\n",
    "                acce_max[:] = acce_data[0], acce_mag_filt\n",
    "            elif (state_flag == 2) and ((acce_data[0] - acce_max[0]) > interval_threshold):\n",
    "                acce_max[:] = acce_data[0], acce_mag_filt\n",
    "                state_flag = 1\n",
    "\n",
    "        # choose reasonable step criterion and check if there is a valid step\n",
    "        # save step acceleration data: step_acce_max_mins = [timestamp, max, min, variance]\n",
    "        step_flag = False\n",
    "        if step_criterion == 2:\n",
    "            if (acce_binarys[-1] == -1) and ((acce_binarys[-2] == 1) or (acce_binarys[-2] == 0)):\n",
    "                step_flag = True\n",
    "        elif step_criterion == 3:\n",
    "            if (acce_binarys[-1] == -1) and (acce_binarys[-2] == 0) and (np.sum(acce_binarys[:-2]) > 1):\n",
    "                step_flag = True\n",
    "        else:\n",
    "            if (acce_binarys[-1] == 0) and acce_binarys[-2] == -1:\n",
    "                if (state_flag == 1) and ((acce_data[0] - acce_min[0]) > interval_threshold):\n",
    "                    acce_min[:] = acce_data[0], acce_mag_filt\n",
    "                    state_flag = 2\n",
    "                    step_flag = True\n",
    "                elif (state_flag == 2) and ((acce_data[0] - acce_min[0]) <= interval_threshold) and (\n",
    "                        acce_mag_filt < acce_min[1]):\n",
    "                    acce_min[:] = acce_data[0], acce_mag_filt\n",
    "        if step_flag:\n",
    "            step_timestamps = np.append(step_timestamps, acce_data[0])\n",
    "            step_indexs = np.append(step_indexs, [i])\n",
    "            step_acce_max_mins = np.append(step_acce_max_mins,\n",
    "                                           [[acce_data[0], acce_max[1], acce_min[1], acce_std ** 2]], axis=0)\n",
    "        acce_mag_pre = acce_mag_filt_detrend\n",
    "\n",
    "    return step_timestamps, step_indexs, step_acce_max_mins\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1a19b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stride_length(step_acce_max_mins):\n",
    "    K = 0.4\n",
    "    K_max = 0.8\n",
    "    K_min = 0.4\n",
    "    para_a0 = 0.21468084\n",
    "    para_a1 = 0.09154517\n",
    "    para_a2 = 0.02301998\n",
    "\n",
    "    stride_lengths = np.zeros((step_acce_max_mins.shape[0], 2))\n",
    "    k_real = np.zeros((step_acce_max_mins.shape[0], 2))\n",
    "    step_timeperiod = np.zeros((step_acce_max_mins.shape[0] - 1, ))\n",
    "    stride_lengths[:, 0] = step_acce_max_mins[:, 0]\n",
    "    window_size = 2\n",
    "    step_timeperiod_temp = np.zeros((0, ))\n",
    "\n",
    "    # calculate every step period - step_timeperiod unit: second\n",
    "    for i in range(0, step_timeperiod.shape[0]):\n",
    "        step_timeperiod_data = (\n",
    "            step_acce_max_mins[i + 1, 0] - step_acce_max_mins[i, 0]) / 1000\n",
    "        step_timeperiod_temp = np.append(\n",
    "            step_timeperiod_temp, [step_timeperiod_data])\n",
    "        if step_timeperiod_temp.shape[0] > window_size:\n",
    "            step_timeperiod_temp = np.delete(step_timeperiod_temp, [0])\n",
    "        step_timeperiod[i] = np.sum(\n",
    "            step_timeperiod_temp) / step_timeperiod_temp.shape[0]\n",
    "\n",
    "    # calculate parameters by step period and acceleration magnitude variance\n",
    "    k_real[:, 0] = step_acce_max_mins[:, 0]\n",
    "    k_real[0, 1] = K\n",
    "    for i in range(0, step_timeperiod.shape[0]):\n",
    "        k_real[i + 1, 1] = np.max([(para_a0 + para_a1 / step_timeperiod[i] +\n",
    "                                    para_a2 * step_acce_max_mins[i, 3]), K_min])\n",
    "        k_real[i + 1, 1] = np.min([k_real[i + 1, 1], K_max]) * (K / K_min)\n",
    "\n",
    "    # calculate every stride length by parameters and max and min data of acceleration magnitude\n",
    "    stride_lengths[:, 1] = np.max([(step_acce_max_mins[:, 1] - step_acce_max_mins[:, 2]),\n",
    "                                   np.ones((step_acce_max_mins.shape[0], ))], axis=0)**(1 / 4) * k_real[:, 1]\n",
    "\n",
    "    return stride_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e49ba29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_headings(ahrs_datas):\n",
    "    headings = np.zeros((np.size(ahrs_datas, 0), 2))\n",
    "    for i in np.arange(0, np.size(ahrs_datas, 0)):\n",
    "        ahrs_data = ahrs_datas[i, :]\n",
    "        rot_mat = get_rotation_matrix_from_vector(ahrs_data[1:])\n",
    "        azimuth, pitch, roll = get_orientation(rot_mat)\n",
    "        around_z = (-azimuth) % (2 * np.pi)\n",
    "        headings[i, :] = ahrs_data[0], around_z\n",
    "    return headings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d10aa23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_step_heading(step_timestamps, headings):\n",
    "    step_headings = np.zeros((len(step_timestamps), 2))\n",
    "    step_timestamps_index = 0\n",
    "    for i in range(0, len(headings)):\n",
    "        if step_timestamps_index < len(step_timestamps):\n",
    "            if headings[i, 0] == step_timestamps[step_timestamps_index]:\n",
    "                step_headings[step_timestamps_index, :] = headings[i, :]\n",
    "                step_timestamps_index += 1\n",
    "        else:\n",
    "            break\n",
    "    assert step_timestamps_index == len(step_timestamps)\n",
    "\n",
    "    return step_headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec15b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rel_positions(stride_lengths, step_headings):\n",
    "    rel_positions = np.zeros((stride_lengths.shape[0], 3))\n",
    "    for i in range(0, stride_lengths.shape[0]):\n",
    "        rel_positions[i, 0] = stride_lengths[i, 0]\n",
    "        rel_positions[i, 1] = -stride_lengths[i, 1] * \\\n",
    "            np.sin(step_headings[i, 1])\n",
    "        rel_positions[i, 2] = stride_lengths[i, 1] * \\\n",
    "            np.cos(step_headings[i, 1])\n",
    "\n",
    "    return rel_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7e2d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_step_positions(acce_datas, ahrs_datas, posi_datas):\n",
    "    step_timestamps, step_indexs, step_acce_max_mins = compute_steps(\n",
    "        acce_datas)\n",
    "    headings = compute_headings(ahrs_datas)\n",
    "    stride_lengths = compute_stride_length(step_acce_max_mins)\n",
    "    step_headings = compute_step_heading(step_timestamps, headings)\n",
    "    rel_positions = compute_rel_positions(stride_lengths, step_headings)\n",
    "    step_positions = correct_positions(rel_positions, posi_datas)\n",
    "    return step_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8fdde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 3\n",
    "fs = 50.0  # sample rate, Hz\n",
    "# fs = 100\n",
    "# cutoff = 3.667  # desired cutoff frequency of the filter, Hz\n",
    "cutoff = 3\n",
    "\n",
    "step_distance = 0.8\n",
    "w_height = 1.7\n",
    "m_trans = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ae4604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ea50979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5de686f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_accel_threshold(data, timestamps, threshold):\n",
    "    d_acc = []\n",
    "    last_state = 'below'\n",
    "    crest_troughs = 0\n",
    "    crossings = []\n",
    "\n",
    "    for i, datum in enumerate(data):\n",
    "\n",
    "        current_state = last_state\n",
    "        if datum < threshold:\n",
    "            current_state = 'below'\n",
    "        elif datum > threshold:\n",
    "            current_state = 'above'\n",
    "\n",
    "        if current_state is not last_state:\n",
    "            if current_state is 'above':\n",
    "                crossing = [timestamps[i], threshold]\n",
    "                crossings.append(crossing)\n",
    "            else:\n",
    "                crossing = [timestamps[i], threshold]\n",
    "                crossings.append(crossing)\n",
    "\n",
    "            crest_troughs += 1\n",
    "        last_state = current_state\n",
    "    return np.array(crossings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a919d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steps_compute_rel_positions(acce, magn):\n",
    "\n",
    "    mix_acce = np.sqrt(acce[:, 1:2]**2 + acce[:, 2:3]**2 + acce[:, 3:4]**2)\n",
    "    mix_acce = np.concatenate([acce[:, 0:1], mix_acce], 1)\n",
    "    mix_df = pd.DataFrame(mix_acce)\n",
    "    mix_df.columns = [\"timestamp\", \"acce\"]\n",
    "\n",
    "    filtered = butter_lowpass_filter(mix_df[\"acce\"], cutoff, fs, order)\n",
    "\n",
    "    threshold = filtered.mean() * 1.1\n",
    "    crossings = peak_accel_threshold(filtered, mix_df[\"timestamp\"], threshold)\n",
    "\n",
    "    step_sum = len(crossings)/2\n",
    "    distance = w_height * 0.4 * step_sum\n",
    "\n",
    "    mag_df = pd.DataFrame(magn)\n",
    "    mag_df.columns = [\"timestamp\", \"x\", \"y\", \"z\"]\n",
    "\n",
    "    acce_df = pd.DataFrame(acce)\n",
    "    acce_df.columns = [\"timestamp\", \"ax\", \"ay\", \"az\"]\n",
    "\n",
    "    mag_df = pd.merge(mag_df, acce_df, on=\"timestamp\")\n",
    "    mag_df.dropna()\n",
    "\n",
    "    time_di_list = []\n",
    "\n",
    "    for i in mag_df.iterrows():\n",
    "\n",
    "        gx, gy, gz = i[1][1], i[1][2], i[1][3]\n",
    "        ax, ay, az = i[1][4], i[1][5], i[1][6]\n",
    "\n",
    "        roll = math.atan2(ay, az)\n",
    "        pitch = math.atan2(-1*ax, (ay * math.sin(roll) + az * math.cos(roll)))\n",
    "\n",
    "        q = m_trans - math.degrees(math.atan2(\n",
    "            (gz*math.sin(roll)-gy*math.cos(roll)), (gx*math.cos(pitch) + gy *\n",
    "                                                    math.sin(roll)*math.sin(pitch) + gz*math.sin(pitch)*math.cos(roll))\n",
    "        )) - 90\n",
    "        if q <= 0:\n",
    "            q += 360\n",
    "        time_di_list.append((i[1][0], q))\n",
    "\n",
    "    d_list = [x[1] for x in time_di_list]\n",
    "\n",
    "    steps = []\n",
    "    step_time = []\n",
    "    di_dict = dict(time_di_list)\n",
    "\n",
    "    for n, i in enumerate(crossings[:, :1]):\n",
    "        if n % 2 == 1:\n",
    "            continue\n",
    "        direct_now = di_dict[i[0]]\n",
    "        dx = math.sin(math.radians(direct_now))\n",
    "        dy = math.cos(math.radians(direct_now))\n",
    "#         print(int(n/2+1),\"/x:\",dx,\"/y:\",dy,\"/\",direct_now)\n",
    "        steps.append((i[0], dx, dy))\n",
    "        step_time.append(i[0])\n",
    "\n",
    "        step_dtime = np.diff(step_time)/1000\n",
    "        step_dtime = step_dtime.tolist()\n",
    "        step_dtime.insert(0, 5)\n",
    "\n",
    "        rel_position = []\n",
    "\n",
    "        wp_idx = 0\n",
    "#         print(\"WP:\",round(sample_file.waypoint[0,1],3),round(sample_file.waypoint[0,2],3),sample_file.waypoint[0,0])\n",
    "#         print(\"------------------\")\n",
    "        for p, i in enumerate(steps):\n",
    "            step_distance = 0\n",
    "            if step_dtime[p] >= 1:\n",
    "                step_distance = w_height*0.25\n",
    "            elif step_dtime[p] >= 0.75:\n",
    "                step_distance = w_height*0.3\n",
    "            elif step_dtime[p] >= 0.5:\n",
    "                step_distance = w_height*0.4\n",
    "            elif step_dtime[p] >= 0.35:\n",
    "                step_distance = w_height*0.45\n",
    "            elif step_dtime[p] >= 0.2:\n",
    "                step_distance = w_height*0.5\n",
    "            else:\n",
    "                step_distance = w_height*0.4\n",
    "\n",
    "#             step_x += i[1]*step_distance\n",
    "#             step_y += i[2]*step_distance\n",
    "\n",
    "            rel_position.append([i[0], i[1]*step_distance, i[2]*step_distance])\n",
    "#     print(rel_position)\n",
    "\n",
    "    return np.array(rel_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65def58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_process(sub, train_waypoints):\n",
    "    sub = split_col(sub[['site_path_timestamp','floor','x','y']]).copy()\n",
    "    sub = sub.merge(train_waypoints[['site','floorNo','floor']].drop_duplicates(), how='left')\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b092cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_col(df):\n",
    "    df = pd.concat([\n",
    "        df['site_path_timestamp'].str.split('_', expand=True) \\\n",
    "        .rename(columns={0:'site',\n",
    "                         1:'path',\n",
    "                         2:'timestamp'}),\n",
    "        df\n",
    "    ], axis=1).copy()\n",
    "    return df\n",
    "\n",
    "floor_map = {\"B2\":-2, \"B1\":-1, \"F1\":0, \"F2\": 1, \"F3\":2,\n",
    "             \"F4\":3, \"F5\":4, \"F6\":5, \"F7\":6,\"F8\":7,\"F9\":8,\n",
    "             \"1F\":0, \"2F\":1, \"3F\":2, \"4F\":3, \"5F\":4, \"6F\":5,\n",
    "             \"7F\":6, \"8F\": 7, \"9F\":8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d36663b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds(\n",
    "    ax,\n",
    "    context_text,\n",
    "    site,\n",
    "    floorNo,\n",
    "    sub=None,\n",
    "    true_locs=None,\n",
    "    base=\"../input/indoor-location-navigation\",\n",
    "    show_train=True,\n",
    "    show_preds=True,\n",
    "    fix_labels=True,\n",
    "    map_floor=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots predictions on floorplan map.\n",
    "    \n",
    "    map_floor : use a different floor's map\n",
    "    \"\"\"\n",
    "    if map_floor is None:\n",
    "        map_floor = floorNo\n",
    "    # Prepare width_meter & height_meter (taken from the .json file)\n",
    "    floor_plan_filename = f\"{base}/metadata/{site}/{map_floor}/floor_image.png\"\n",
    "    json_plan_filename = f\"{base}/metadata/{site}/{map_floor}/floor_info.json\"\n",
    "    with open(json_plan_filename) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    width_meter = json_data[\"map_info\"][\"width\"]\n",
    "    height_meter = json_data[\"map_info\"][\"height\"]\n",
    "\n",
    "    floor_img = plt.imread(f\"{base}/metadata/{site}/{map_floor}/floor_image.png\")\n",
    "\n",
    "\n",
    "    ax.imshow(floor_img)\n",
    "\n",
    "    if show_train:\n",
    "        true_locs = true_locs.query('site == @site and floorNo == @map_floor').copy()\n",
    "        true_locs[\"x_\"] = true_locs[\"x\"] * floor_img.shape[0] / height_meter\n",
    "        true_locs[\"y_\"] = (\n",
    "            true_locs[\"y\"] * -1 * floor_img.shape[1] / width_meter\n",
    "        ) + floor_img.shape[0]\n",
    "        true_locs.query(\"site == @site and floorNo == @map_floor\").groupby(\"path\").plot(\n",
    "            x=\"x_\",\n",
    "            y=\"y_\",\n",
    "            style=\"+\",\n",
    "            ax=ax,\n",
    "            label=\"train waypoint location\",\n",
    "            color=\"grey\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "\n",
    "    if show_preds:\n",
    "        sub = sub.query('site == @site and floorNo == @floorNo').copy()\n",
    "        sub[\"x_\"] = sub[\"x\"] * floor_img.shape[0] / height_meter\n",
    "        sub[\"y_\"] = (\n",
    "            sub[\"y\"] * -1 * floor_img.shape[1] / width_meter\n",
    "        ) + floor_img.shape[0]\n",
    "        for path, path_data in sub.query(\n",
    "            \"site == @site and floorNo == @floorNo\"\n",
    "        ).groupby(\"path\"):\n",
    "            path_data.plot(\n",
    "                x=\"x_\",\n",
    "                y=\"y_\",\n",
    "                style=\".-\",\n",
    "                ax=ax,\n",
    "                title=f\"{context_text} - {site} - floor - {floorNo}\",\n",
    "                alpha=1,\n",
    "                label=path,\n",
    "            )\n",
    "    if fix_labels:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        by_label = dict(zip(labels, handles))\n",
    "        ax.legend(\n",
    "            by_label.values(), by_label.keys(), loc=\"center left\", bbox_to_anchor=(1, 0.5)\n",
    "        )\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "530642d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target_sites(sub_df):\n",
    "    return sorted(sub_df['site'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fcd04628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_site_floors_dict(sub_df):\n",
    "    sites = generate_target_sites(sub_df)\n",
    "    site_floors_dict = {}\n",
    "    for site in sites:\n",
    "        site_path = Path('/kaggle/input/indoor-location-navigation/train') / site\n",
    "        site_floors_dict[site] = [path.name for path in site_path.glob('*')]\n",
    "    return site_floors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1a6e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sites(sites, sub1_df, sub2_df, sub3_df):\n",
    "    num_floors = 0\n",
    "    for site in sites:\n",
    "        num_floors += len(site_floors_dict[site])\n",
    "\n",
    "    fig, ax = plt.subplots(num_floors, 3, figsize=(42, 12 * num_floors))\n",
    "\n",
    "    idx = 0\n",
    "    for site in sites:\n",
    "        floors = site_floors_dict[site]\n",
    "\n",
    "        for floor in floors:\n",
    "            plot_preds(ax[idx][0], \"raw submission\", site, floor, sub1_df, train_waypoints, show_preds=True)\n",
    "            plot_preds(ax[idx][1], \"Postprocess without augmentation\", site, floor, sub2_df, train_waypoints, show_preds=True)\n",
    "            plot_preds(ax[idx][2], \"Postprocess with augmentation\", site, floor, sub3_df, train_waypoints, show_preds=True)\n",
    "            idx += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c479da3",
   "metadata": {},
   "source": [
    "\n",
    "# Generate augmented waypoints\n",
    "\n",
    "    Augmented waypoints have similar X and Y values of train ones.\n",
    "    Augmented waypoints are in hallways.\n",
    "    Augmented waypoints are sufficiently distant from train ones and each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a47acbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_array(array, floor_max, floor_min, site_max):\n",
    "    array=(array-floor_min)/(floor_max-floor_min)* site_max\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc557924",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_polygon_dict=False\n",
    "if create_polygon_dict:\n",
    "\n",
    "    polygon_dict=defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: Polygon([]))))\n",
    "    for i in tqdm(range(len(train_meta))):\n",
    "        t = train_meta.iloc[i]\n",
    "        n_floor = FeatureStore.floor_convert[t.floor]\n",
    "        if polygon_dict[t.site_id][n_floor]['floor_polygons'] == Polygon([]):\n",
    "            feature = pickle_load_dill(out_dir+'features/'+path_id+'.pickle')\n",
    "            floor_arrays=np.zeros([0,2])\n",
    "            for j in range(len(feature.site_info.floor_coordinates)):\n",
    "                floor_array=np.array(feature.site_info.floor_coordinates[j]).squeeze()\n",
    "                floor_arrays=np.vstack([floor_arrays,floor_array])\n",
    "            floor_min=floor_arrays.min(axis=0)\n",
    "            floor_max=floor_arrays.max(axis=0)\n",
    "            site_max=np.array([feature.site_info.site_width,feature.site_info.site_height])\n",
    "\n",
    "            floor_polygons=[]\n",
    "            for j in range(len(feature.site_info.floor_coordinates)):\n",
    "                floor_array=np.array(feature.site_info.floor_coordinates[j]).squeeze()\n",
    "                floor_array = normalize_array(floor_array, floor_max, floor_min, site_max)\n",
    "                floor_polygon = Polygon(floor_array)\n",
    "                floor_polygons.append(floor_polygon)\n",
    "            floor_polygons= so.unary_union(floor_polygons)\n",
    "\n",
    "            store_polygons=[]\n",
    "            for j in range(len(feature.site_info.store_coordinates)):\n",
    "                for k in range(len(feature.site_info.store_coordinates[j])):\n",
    "                    store_array=np.array(feature.site_info.store_coordinates[j][k]).squeeze()\n",
    "                    store_array = normalize_array(store_array, floor_max, floor_min, site_max)\n",
    "                    store_polygon  = Polygon(store_array)\n",
    "                    store_polygons.append(store_polygon)\n",
    "            store_polygons= so.unary_union(store_polygons)\n",
    "\n",
    "            polygon_dict[t.site_id][feature.n_floor]['floor_polygons']=floor_polygons\n",
    "            polygon_dict[t.site_id][feature.n_floor]['store_polygons']=store_polygons\n",
    "else:\n",
    "    polygon_dict=pickle_load_dill('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df90fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_arr(arr, threshold=1):\n",
    "    new_arr=[]\n",
    "    old_x=-9999\n",
    "    temp_list=[]\n",
    "    for x in np.sort(arr):\n",
    "        if len(temp_list)>0:\n",
    "            temp_mean=np.concatenate([temp_list]).mean()\n",
    "            if x-temp_mean > threshold:\n",
    "                new_arr.append(temp_mean)\n",
    "                temp_list=[]\n",
    "        temp_list.append(x)\n",
    "    if len(temp_list)>0:\n",
    "        temp_mean=np.concatenate([temp_list]).mean()\n",
    "        new_arr.append(temp_mean)\n",
    "        \n",
    "    new_arr=np.concatenate([new_arr])\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65a8029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rot_arr(arr, deg):\n",
    "    rad=np.deg2rad(deg)\n",
    "    rot_mat = np.array([[np.cos(rad), -np.sin(rad)],\n",
    "                  [np.sin(rad),  np.cos(rad)]])\n",
    "    return  np.dot(arr,rot_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "203873ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_aug_wps(wps,site_id,n_floor,polygon_dict,floor_margin=0,store_margin=0,dist_threshold=4,dist_to_wps_threshold=6,grid_threshold=1,num_arr=300, rot_grid=True, aug_generator='grid'):\n",
    "    if aug_generator=='random':\n",
    "        aug_wps=np.vstack([np.random.random_sample((num_arr))*(wps.max(axis=0)[0]-wps.min(axis=0)[0])+wps.min(axis=0)[0],np.random.random_sample((num_arr))*(wps.max(axis=0)[1]-wps.min(axis=0)[1])+wps.min(axis=0)[1]]).transpose()\n",
    "    if aug_generator=='grid':\n",
    "        if rot_grid:\n",
    "            min_num_grid=np.inf\n",
    "            for angle in range(90):\n",
    "                rot_wps=rot_arr(wps, angle)\n",
    "                wps_x=subsample_arr(rot_wps[:,0], threshold=grid_threshold)\n",
    "                wps_y=subsample_arr(rot_wps[:,1], threshold=grid_threshold)\n",
    "                len_wps_x=len(wps_x)\n",
    "                len_wps_y=len(wps_y)\n",
    "                num_grid=len_wps_x*len_wps_y\n",
    "                if num_grid<min_num_grid:\n",
    "                    min_num_grid=num_grid\n",
    "                    min_angle=angle\n",
    "            rot_wps=rot_arr(wps, min_angle)\n",
    "            wps_x=subsample_arr(rot_wps[:,0], threshold=grid_threshold)\n",
    "            wps_y=subsample_arr(rot_wps[:,1], threshold=grid_threshold)\n",
    "            len_wps_x=len(wps_x)\n",
    "            len_wps_y=len(wps_y)\n",
    "            aug_wps=np.vstack([np.tile(wps_x, (len_wps_y)),np.repeat(wps_y, len_wps_x, axis=0)]).transpose()\n",
    "            aug_wps=rot_arr(aug_wps, -min_angle)\n",
    "        else:\n",
    "            wps_x=subsample_arr(wps[:,0], threshold=grid_threshold)\n",
    "            wps_y=subsample_arr(wps[:,1], threshold=grid_threshold)\n",
    "            len_wps_x=len(wps_x)\n",
    "            len_wps_y=len(wps_y)\n",
    "            aug_wps=np.vstack([np.tile(wps_x, (len_wps_y)),np.repeat(wps_y, len_wps_x, axis=0)]).transpose()\n",
    "    if aug_generator=='triangle':\n",
    "        aug_wps=wps_sub.copy()\n",
    "        tri = Delaunay(aug_wps)\n",
    "        aug_wps=(np.vstack(aug_wps[tri.simplices])+np.vstack(np.roll(aug_wps[tri.simplices],1, axis=1)))/2\n",
    "        idx = np.random.randint(len(aug_wps), size=100)\n",
    "        aug_wps=aug_wps[idx,:]\n",
    "        getunique\n",
    "        sorted_idx = np.lexsort(aug_wps.T)\n",
    "        sorted_data =  aug_wps[sorted_idx,:]\n",
    "        # Get unique row mask\n",
    "        row_mask = np.append([True],np.any(np.diff(sorted_data,axis=0),1))\n",
    "        # Get unique rows\n",
    "        aug_wps = sorted_data[row_mask]   \n",
    "    floor_polygons=polygon_dict[site_id][n_floor]['floor_polygons']\n",
    "    store_polygons=polygon_dict[site_id][n_floor]['store_polygons']\n",
    "    floor_polygons_eroded = floor_polygons.buffer(-floor_margin)\n",
    "    store_polygons_dilated = store_polygons.buffer(store_margin)\n",
    "    safe_area_polygons=floor_polygons_eroded.difference(store_polygons_dilated)\n",
    "    mask_arr=np.array([True]*len(aug_wps))\n",
    "    for cnt, row in enumerate(aug_wps):\n",
    "        x = float(row[0])\n",
    "        y = float(row[1])\n",
    "        point = Point([x,y])\n",
    "        if not safe_area_polygons.contains(point):\n",
    "            mask_arr[cnt]=False\n",
    "    #print(aug_wps.shape)\n",
    "    aug_wps=aug_wps[mask_arr]\n",
    "    #print(aug_wps.shape)\n",
    "    \n",
    "    #remove aug wps based on distance to wps\n",
    "    if len(aug_wps)>0:        \n",
    "        dist_aug = distance.cdist(aug_wps, wps, metric='euclidean')\n",
    "        dist_aug=dist_aug.min(axis=1)\n",
    "        #aug_wps=aug_wps[(dist_aug<7)&(dist_aug>2)]\n",
    "        aug_wps=aug_wps[dist_aug>dist_to_wps_threshold]\n",
    "        \n",
    "    #remove aug wps based on distance among aug wps\n",
    "    if len(aug_wps)>0:\n",
    "        dist_aug = distance.cdist(aug_wps, aug_wps, metric='euclidean')\n",
    "        dist_aug = np.tril(dist_aug, k=-1)\n",
    "        dist_aug = np.where(dist_aug == 0, np.inf, dist_aug) \n",
    "        for i in range(len(dist_aug)):\n",
    "            if dist_aug[i,:].min() < dist_threshold:\n",
    "                dist_aug[:,i]=np.inf    \n",
    "        dist_aug=dist_aug.min(axis=1)\n",
    "        #aug_wps=aug_wps[(dist_aug<7)&(dist_aug>2)]\n",
    "        aug_wps=aug_wps[dist_aug>dist_threshold]\n",
    "        \n",
    "    #print(aug_wps.shape)\n",
    "    return aug_wps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e65d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "wps_dict = pickle_load_dill(mydata_dir+'wps_dict.pickle')\n",
    "train_meta_sub = pickle_load_dill(mydata_dir+'train_meta_sub.pickle')\n",
    "test_meta = pickle_load_dill(mydata_dir+'test_meta.pickle')\n",
    "#hlwps_df = pd.read_csv('../input/indoor-navigation-hand-labeled-waypoints/waypoint_by_hand.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f0954",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_aug_wps_dict=True\n",
    "if create_aug_wps_dict:\n",
    "    aug_wps_dict=defaultdict(lambda: defaultdict(lambda: np.empty([0, 2])))\n",
    "    for site_id in tqdm(wps_dict.keys()):\n",
    "        for n_floor in wps_dict[site_id].keys():\n",
    "            wps=wps_dict[site_id][n_floor]\n",
    "            aug_wps_dict[site_id][n_floor] = generate_aug_wps(wps,site_id,n_floor,polygon_dict,floor_margin=0,store_margin=0,dist_threshold=4,dist_to_wps_threshold=6,grid_threshold=1,rot_grid=False,num_arr=300, aug_generator='grid')\n",
    "    pickle_dump_dill(aug_wps_dict,'aug_wps_dict.pickle')\n",
    "else:\n",
    "    aug_wps_dict = pickle_load_dill(mydata_dir+'aug_wps_dict.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93468466",
   "metadata": {},
   "source": [
    "# Correct floor prediction based on the leakages of shared wifi records and device IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6767565c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mydata_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m         pickle_dump_dill(count_rank_user_id_dict, out_dir \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    251\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_rank_user_id_dict.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     df_user \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mmydata_dir\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_user_id.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mydata_dir' is not defined"
     ]
    }
   ],
   "source": [
    "create_df_user = False\n",
    "if create_df_user:\n",
    "    print('create_df_user')\n",
    "    train_meta['start_time'] = 0\n",
    "    train_meta['end_time'] = 0\n",
    "    train_meta['start_wp_time'] = 0\n",
    "    train_meta['start_wp_x'] = 0\n",
    "    train_meta['start_wp_y'] = 0\n",
    "    train_meta['end_wp_time'] = 0\n",
    "    train_meta['end_wp_x'] = 0\n",
    "    train_meta['end_wp_y'] = 0\n",
    "    train_meta['n_floor'] = 0\n",
    "    wifi_dict = defaultdict(lambda: pd.DataFrame())\n",
    "    mag_unc_dict = defaultdict(lambda: pd.DataFrame())\n",
    "    gyr_unc_dict = defaultdict(lambda: pd.DataFrame())\n",
    "    acc_unc_dict = defaultdict(lambda: pd.DataFrame())\n",
    "    for i in tqdm(range(len(train_meta))):\n",
    "        t = train_meta.iloc[i]\n",
    "        feature = pickle_load_dill(out_dir+'feature/'+t.path_id+'.pickle')\n",
    "        if feature.meta_info_df[feature.meta_info_df[0] == 'startTime'][1].values == None:\n",
    "            start_time = int(np.nanmin([feature.accelerometer.timestamp.min(\n",
    "            ), feature.wifi.timestamp.min(), feature.beacon.timestamp.min()]))\n",
    "        else:\n",
    "            start_time = int(\n",
    "                feature.meta_info_df[feature.meta_info_df[0] == 'startTime'][1])\n",
    "        if (len(feature.meta_info_df[feature.meta_info_df[0] == 'endTime']) == 0) or (feature.meta_info_df[feature.meta_info_df[0] == 'endTime'][1].values == None):\n",
    "            end_time = int(np.nanmax([feature.accelerometer.timestamp.max(\n",
    "            ), feature.wifi.timestamp.max(), feature.beacon.timestamp.max()]))\n",
    "        else:\n",
    "            end_time = int(\n",
    "                feature.meta_info_df[feature.meta_info_df[0] == 'endTime'][1])\n",
    "        waypoint_df = feature.waypoint.copy()\n",
    "        train_meta.loc[i, 'start_time'] = start_time\n",
    "        train_meta.loc[i,\n",
    "                       'start_wp_time'] = waypoint_df.iloc[0]['timestamp']\n",
    "        train_meta.loc[i, 'start_wp_x'] = waypoint_df.iloc[0]['x']\n",
    "        train_meta.loc[i, 'start_wp_y'] = waypoint_df.iloc[0]['y']\n",
    "        train_meta.loc[i, 'end_time'] = end_time\n",
    "        train_meta.loc[i,\n",
    "                       'end_wp_time'] = waypoint_df.iloc[-1]['timestamp']\n",
    "        train_meta.loc[i, 'end_wp_x'] = waypoint_df.iloc[-1]['x']\n",
    "        train_meta.loc[i, 'end_wp_y'] = waypoint_df.iloc[-1]['y']\n",
    "        train_meta.loc[i, 'n_floor'] = feature.n_floor\n",
    "\n",
    "        wifi_dict[t.path_id] = feature.wifi[[\n",
    "            'bssid', 'last_seen_timestamp']].drop_duplicates()\n",
    "        if 'x2' in feature.accelerometer_uncalibrated.columns:\n",
    "            acc_unc_dict[t.path_id] = feature.accelerometer_uncalibrated[[\n",
    "                'x2', 'y2', 'z2']].drop_duplicates()\n",
    "        if 'x2' in feature.gyroscope_uncalibrated.columns:\n",
    "            gyr_unc_dict[t.path_id] = feature.gyroscope_uncalibrated[[\n",
    "                'x2', 'y2', 'z2']].drop_duplicates()\n",
    "        if 'x2' in feature.magnetic_field_uncalibrated.columns:\n",
    "            mag_unc_dict[t.path_id] = feature.magnetic_field_uncalibrated[[\n",
    "                'x2', 'y2', 'z2']].drop_duplicates()\n",
    "    train_meta = train_meta.sort_values(\n",
    "        ['site_id', 'start_time']).reset_index(drop=True)\n",
    "\n",
    "    test_meta['start_time'] = 0\n",
    "    test_meta['end_time'] = 0\n",
    "    test_meta['start_wp_time'] = 0\n",
    "    test_meta['start_wp_x'] = 0\n",
    "    test_meta['start_wp_y'] = 0\n",
    "    test_meta['end_wp_time'] = 0\n",
    "    test_meta['end_wp_x'] = 0\n",
    "    test_meta['end_wp_y'] = 0\n",
    "    test_meta['n_floor'] = 0\n",
    "    for i in tqdm(range(len(test_meta))):\n",
    "        t = test_meta.iloc[i]\n",
    "        #print(f\"site_id: {t.site_id}, floor: {t.floor}, path_id: {t.path_id}\")\n",
    "        feature = pickle_load_dill(out_dir+'feature/'+t.path_id+'.pickle')\n",
    "        if feature.meta_info_df[feature.meta_info_df[0] == 'startTime'][1].values == None:\n",
    "            start_time = int(np.nanmin([feature.accelerometer.timestamp.min(\n",
    "            ), feature.wifi.timestamp.min(), feature.beacon.timestamp.min()]))\n",
    "        else:\n",
    "            start_time = int(\n",
    "                feature.meta_info_df[feature.meta_info_df[0] == 'startTime'][1])\n",
    "        if (len(feature.meta_info_df[feature.meta_info_df[0] == 'endTime']) == 0) or (feature.meta_info_df[feature.meta_info_df[0] == 'endTime'][1].values == None):\n",
    "            end_time = int(np.nanmax([feature.accelerometer.timestamp.max(\n",
    "            ), feature.wifi.timestamp.max(), feature.beacon.timestamp.max()]))\n",
    "        else:\n",
    "            end_time = int(\n",
    "                feature.meta_info_df[feature.meta_info_df[0] == 'endTime'][1])\n",
    "        if len(feature.beacon) > 0:\n",
    "            gap = feature.beacon.loc[0, 'timestamp2'] - \\\n",
    "                feature.beacon.loc[0, 'timestamp']\n",
    "        else:\n",
    "            gap = (feature.wifi.last_seen_timestamp.values -\n",
    "                   feature.wifi.timestamp.values).max()+324.5229450792481  # from mean gap\n",
    "        waypoint_df = feature.waypoint.copy()\n",
    "        test_meta.loc[i, 'start_time'] = start_time+gap\n",
    "        test_meta.loc[i,\n",
    "                      'start_wp_time'] = waypoint_df.iloc[0]['timestamp']\n",
    "        test_meta.loc[i, 'start_wp_x'] = waypoint_df.iloc[0]['x']\n",
    "        test_meta.loc[i, 'start_wp_y'] = waypoint_df.iloc[0]['y']\n",
    "        test_meta.loc[i, 'end_time'] = end_time+gap\n",
    "        test_meta.loc[i,\n",
    "                      'end_wp_time'] = waypoint_df.iloc[-1]['timestamp']\n",
    "        test_meta.loc[i, 'end_wp_x'] = waypoint_df.iloc[-1]['x']\n",
    "        test_meta.loc[i, 'end_wp_y'] = waypoint_df.iloc[-1]['y']\n",
    "        test_meta.loc[i, 'n_floor'] = feature.n_floor\n",
    "\n",
    "        wifi_dict[t.path_id] = feature.wifi[[\n",
    "            'bssid', 'last_seen_timestamp']].drop_duplicates()\n",
    "        if 'x2' in feature.accelerometer_uncalibrated.columns:\n",
    "            acc_unc_dict[t.path_id] = feature.accelerometer_uncalibrated[[\n",
    "                'x2', 'y2', 'z2']].drop_duplicates()\n",
    "        if 'x2' in feature.gyroscope_uncalibrated.columns:\n",
    "            gyr_unc_dict[t.path_id] = feature.gyroscope_uncalibrated[[\n",
    "                'x2', 'y2', 'z2']].drop_duplicates()\n",
    "        if 'x2' in feature.magnetic_field_uncalibrated.columns:\n",
    "            mag_unc_dict[t.path_id] = feature.magnetic_field_uncalibrated[[\n",
    "                'x2', 'y2', 'z2']].drop_duplicates()\n",
    "\n",
    "    df_user = pd.merge(train_meta, test_meta, how='outer')\n",
    "    df_user = df_user.sort_values(\n",
    "        ['site_id', 'start_time']).reset_index(drop=True)\n",
    "    print('create_user_id')\n",
    "    threshold = 0.00001\n",
    "    df_user['user_id_wifi'] = 0\n",
    "    df_user['counter_wifi'] = 0\n",
    "    df_user['user_id_acc'] = 0\n",
    "    df_user['counter_acc'] = 0\n",
    "    df_user['min_dist_acc'] = 0\n",
    "    df_user['user_id_gyr'] = 0\n",
    "    df_user['counter_gyr'] = 0\n",
    "    df_user['min_dist_gyr'] = 0\n",
    "    df_user['user_id_mag'] = 0\n",
    "    df_user['counter_mag'] = 0\n",
    "    df_user['min_dist_mag'] = 0\n",
    "    n_wifi = 0\n",
    "    n_acc = 0\n",
    "    n_gyr = 0\n",
    "    n_mag = 0\n",
    "    for i in tqdm(range(len(df_user))):\n",
    "        t = df_user.iloc[i]\n",
    "        current_wifi = wifi_dict[t.path_id]\n",
    "        min_last_seen_timestamp = current_wifi.last_seen_timestamp.min()\n",
    "        df_site = df_user[df_user.site_id == t.site_id]\n",
    "        df_site = df_site[df_site.end_time < t.start_time]\n",
    "        df_site = df_site[min_last_seen_timestamp < df_site.end_time]\n",
    "        counter_wifi = 0\n",
    "        counter_acc = 0\n",
    "        counter_gyr = 0\n",
    "        counter_mag = 0\n",
    "        min_dist_acc = np.inf\n",
    "        min_dist_gyr = np.inf\n",
    "        min_dist_mag = np.inf\n",
    "        if len(df_site) > 0:\n",
    "            acc = acc_unc_dict[t.path_id]\n",
    "            mag = mag_unc_dict[t.path_id]\n",
    "            gyr = gyr_unc_dict[t.path_id]\n",
    "            for j in range(len(df_site)):\n",
    "                t = df_site.iloc[j]\n",
    "                old_wifi = wifi_dict[t.path_id]\n",
    "                common_wifi = pd.merge(\n",
    "                    current_wifi, old_wifi, how='inner', on=['bssid'])\n",
    "                common_wifi['diff_time'] = abs(\n",
    "                    common_wifi.last_seen_timestamp_x-common_wifi.last_seen_timestamp_y)\n",
    "                if (common_wifi.diff_time < 5).sum() > 0:\n",
    "                    # If there is a leak\n",
    "                    df_user.loc[i, 'user_id_wifi'] = t.user_id_wifi\n",
    "                    counter_wifi += 1\n",
    "            if len(acc) > 0:\n",
    "                if acc.values.sum() > 0:\n",
    "                    for j in range(len(df_site)):\n",
    "                        t = df_site.iloc[j]\n",
    "                        if len(acc_unc_dict[t.path_id]) > 0:\n",
    "                            dist_M = distance.cdist(\n",
    "                                acc.values, acc_unc_dict[t.path_id].values, metric='euclidean')\n",
    "                            min_dist_acc = np.min([min_dist_acc, dist_M.min()])\n",
    "                            if dist_M.min() < threshold:\n",
    "                                df_user.loc[i, 'user_id_acc'] = t.user_id_acc\n",
    "                                counter_acc += 1\n",
    "            if len(mag) > 0:\n",
    "                for j in range(len(df_site)):\n",
    "                    t = df_site.iloc[j]\n",
    "                    if len(mag_unc_dict[t.path_id]) > 0:\n",
    "                        dist_M = distance.cdist(\n",
    "                            mag.values, mag_unc_dict[t.path_id].values, metric='euclidean')\n",
    "                        min_dist_mag = np.min([min_dist_mag, dist_M.min()])\n",
    "                        if dist_M.min() < threshold:\n",
    "                            df_user.loc[i, 'user_id_mag'] = t.user_id_mag\n",
    "                            counter_mag += 1\n",
    "            if len(gyr) > 0:\n",
    "                for j in range(len(df_site)):\n",
    "                    t = df_site.iloc[j]\n",
    "                    if len(gyr_unc_dict[t.path_id]) > 0:\n",
    "                        dist_M = distance.cdist(\n",
    "                            gyr.values, gyr_unc_dict[t.path_id].values, metric='euclidean')\n",
    "                        min_dist_gyr = np.min([min_dist_gyr, dist_M.min()])\n",
    "                        if dist_M.min() < threshold:\n",
    "                            df_user.loc[i, 'user_id_gyr'] = t.user_id_gyr\n",
    "                            counter_gyr += 1\n",
    "        if counter_wifi == 0:\n",
    "            df_user.loc[i, 'user_id_wifi'] = n_wifi\n",
    "            n_wifi += 1\n",
    "        df_user.loc[i, 'counter_wifi'] = counter_wifi\n",
    "\n",
    "        if counter_acc == 0:\n",
    "            df_user.loc[i, 'user_id_acc'] = n_acc\n",
    "            n_acc += 1\n",
    "        df_user.loc[i, 'counter_acc'] = counter_acc\n",
    "        df_user.loc[i, 'min_dist_acc'] = min_dist_acc\n",
    "\n",
    "        if counter_mag == 0:\n",
    "            df_user.loc[i, 'user_id_mag'] = n_mag\n",
    "            n_mag += 1\n",
    "        df_user.loc[i, 'counter_mag'] = counter_mag\n",
    "        df_user.loc[i, 'min_dist_mag'] = min_dist_mag\n",
    "\n",
    "        if counter_gyr == 0:\n",
    "            df_user.loc[i, 'user_id_gyr'] = n_gyr\n",
    "            n_gyr += 1\n",
    "        df_user.loc[i, 'counter_gyr'] = counter_gyr\n",
    "        df_user.loc[i, 'min_dist_gyr'] = min_dist_gyr\n",
    "\n",
    "    df_user['user_id'] = df_user.user_id_wifi.copy()\n",
    "    for user_id in df_user.user_id_acc.unique():\n",
    "        df_user.loc[df_user.user_id_acc == user_id,\n",
    "                    'user_id'] = df_user.loc[df_user.user_id_acc == user_id, 'user_id'].min()\n",
    "    for user_id in df_user.user_id_mag.unique():\n",
    "        df_user.loc[df_user.user_id_mag == user_id,\n",
    "                    'user_id'] = df_user.loc[df_user.user_id_mag == user_id, 'user_id'].min()\n",
    "    for user_id in df_user.user_id_gyr.unique():\n",
    "        df_user.loc[df_user.user_id_gyr == user_id,\n",
    "                    'user_id'] = df_user.loc[df_user.user_id_gyr == user_id, 'user_id'].min()\n",
    "    for user_id in df_user.user_id_wifi.unique():\n",
    "        df_user.loc[df_user.user_id_wifi == user_id,\n",
    "                    'user_id'] = df_user.loc[df_user.user_id_wifi == user_id, 'user_id'].min()\n",
    "    le = LabelEncoder()\n",
    "    labels = df_user.user_id\n",
    "    le.fit(labels)\n",
    "    df_user.user_id = le.transform(labels)\n",
    "\n",
    "    count_user_id_dict = defaultdict(int)\n",
    "    for cnt, row in enumerate(df_user[['start_wp_x', 'user_id']].values):\n",
    "        start_wp_x = int(row[0])\n",
    "        user_id = int(row[1])\n",
    "        if start_wp_x > 0:\n",
    "            count_user_id_dict[user_id] += 1\n",
    "        else:\n",
    "            count_user_id_dict[user_id] = count_user_id_dict[user_id]\n",
    "    count_rank_user_id_dict = {key: rank for rank, key in enumerate(\n",
    "        sorted(count_user_id_dict, key=count_user_id_dict.get, reverse=True), 1)}\n",
    "    if not cfg.debug:\n",
    "        df_user.to_csv(out_dir + 'df_user_id.csv')\n",
    "        pickle_dump_dill(count_user_id_dict, out_dir +\n",
    "                         'count_user_id_dict.pickle')\n",
    "        pickle_dump_dill(count_rank_user_id_dict, out_dir +\n",
    "                         'count_rank_user_id_dict.pickle')\n",
    "else:\n",
    "    df_user = pd.read_csv(mydata_dir + 'df_user_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811b0274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
